{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postmodel Analysis\n",
    "\n",
    "This notebook will guide the `triage` user through some useful rutines to compare individual models across `model_group_id`'s and also `model_group`s. This is an interactive process that combines huntches and some general ways to compare models. Before starting this process, is important to run `triage`'s _Audition_ component which will filter `model_group_id`'s using user-defined metrics (see [Audition](https://github.com/dssg/triage/blob/master/src/triage/component/audition/Audition_Tutorial.ipynb) to explore more). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from utils.aux_funcs import create_pgconn, get_models_ids\n",
    "from triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine\n",
    "from parameters import PostmodelParameters\n",
    "from model_evaluator import ModelEvaluator\n",
    "from model_group_evaluator import ModelGroupEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Postmodel need a set of parameters to pass through some of its functions. This will allow the user to better analyize its models, This parameters can be readed from a `.yaml` file (a `postmodeling_parameters.yaml` example is included in this Notebook). If you want to set other parameters, you can pass them to the `.yaml` file, and the `PostmodelParameters` class, will create them. \n",
    "\n",
    "You can define de selected models by either defining a path to an `Audition` output, or by adding your own prefered models. \n",
    "\n",
    "This parameters include: \n",
    " - `project_path`: Path to matrices and modes (normally under `triage/output`)\n",
    " - `audition_output_path`: Path to Audition output `.json` file. \n",
    " - `model_group_id`: List with selected `ModelGroup` from `Audition`\n",
    " - `metric`: Selected metric string (i.e. `precision@`,`recall@`)\n",
    " - `thresholds`: Selected threshold list (i.e. `rank_abs[50, 100]`)\n",
    " - `baseline_query`:A SQL query that returns evaluation metrics for the baseline models\n",
    " - `n_features_plots`: Number of features to plot importances (int)\n",
    " - `max_depth_error_trees`: For residual DecisionTrees, number of splits (int)\n",
    "\n",
    "Some aesthetic parameters\n",
    "\n",
    " - `figsize`: Plot size (tuple)\n",
    " - `fontsize`: Font size for plots (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = PostmodelParameters('postmodeling_config_mined.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters stored in the `postmodeling_config.yaml`, and the notebook will use this to set parameters in other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_pgconn('db_credentials_mined.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Model Group Evaluators\n",
    "\n",
    "This class will contain all the individual `model_id`'s (the part of the `model_group_id`) for each of the `model_group_id`'s defined in the paramters configuration file. To instantiate all the models we can create a list with each of the models of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List individual models\n",
    "list_tuple_models = get_models_ids(params.model_group_id, engine)\n",
    "l_t = [ModelEvaluator(i.model_group_id, i.model_id, engine) for i in list_tuple_models]\n",
    "\n",
    "# Model group object (useful to compare across model_groups and models in time)\n",
    "audited_models_class = ModelGroupEvaluator(tuple(params.model_group_id), engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the contents of the `ModelEvaluator` for each `model_id` inside the specified `model_group_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audited_models_class.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's talk about postmodeling:\n",
    "\n",
    "### How the scores looks like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main metrics we want to evaluate from each of our models is their predictions. The way `triage` modeling component works pivot around `model_groups_id` as the main element of modeling. Model Groups have the same model configuration (hyperparameters, feature space, etc) with different models ran in different prediction windows. \n",
    "\n",
    "We want to check a few things when exploring score predictions: \n",
    "\n",
    "1. Score distributions\n",
    "2. Score distributions by trained label\n",
    "\n",
    "Audition will output a dict object with a set of models for each of the defined metrics. We can either read from Audition output, or we can define a list (`audited_models`) to start the post-modeling exploration. We will get each individual `model_id` from the audited `model_groups_id`'s and get the relevant metadata and matrices for each models using the `Model` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[3].plot_score_distribution(figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[3].plot_score_label_distributions(figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[1].plot_score_distribution_thresh(figsize=params.figsize, param_type = 'rank_pct', param = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How's the precision across time for each of my selected model groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, how accurate are these scores? In the paramteres configuration fil we defined a set (or probably one) threshold that will help us to classify (label) predicted entities as 1's or O's. These will help us to assess each of the model groups across time. To have better insights about our model quality, we can compare our model groups against a baseline model (also defined in our configuration file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = audited_models_class.plot_prec_across_time(param_type='rank_pct',\n",
    "                                           param=10,\n",
    "                                           baseline=True,\n",
    "                                           baseline_query=params.baseline_query,\n",
    "                                           metric='precision@',\n",
    "                                           figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = audited_models_class.plot_prec_across_time(param_type='rank_pct',\n",
    "                                           param=10,\n",
    "                                           baseline=True,\n",
    "                                           baseline_query=params.baseline_query,\n",
    "                                           metric='recall@',\n",
    "                                           figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features: what's is happening behind this prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract individual feature importances from `triage` results schema and indentify the important features for each model. In this `dirtyduck` example, we find that a significant set of features are correlated, usually corresponding to imputated values, or `NULL` values in their categorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the model features correlated (and its feature groups)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l_t[3].cluster_correlation_features(path=params.project_path,\n",
    "                                    feature_group_subset_list = ['all']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many zeroes/imputation our model feature space has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[0].cluster_correlation_sparsity(path=params.project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is the feature that has the biggest relevance in my prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[2].plot_feature_importances(path=params.project_path,\n",
    "                                n_features_plots=params.n_features_plots, \n",
    "                                figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l_t[29].plot_feature_importances(path=params.project_path,\n",
    "                                n_features_plots=params.n_features_plots, \n",
    "                                figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_slr = l_t[25]._feature_importance_slr(params.__dict__[\"project_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage_engine = ProjectStorage(params.__dict__[\"project_path\"]).model_storage_engine()\n",
    "classifier = model_storage_engine.load(\"df7d10fa5605c9051bf509187c0f4efa\")\n",
    "coef = classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sq = coef.squeeze()\n",
    "max(c_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(coef)\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"exp\"] = df[0].apply(np.exp)\n",
    "\n",
    "\n",
    "max_fi = []\n",
    "for mh in model_hashes:\n",
    "    model_storage_engine = ProjectStorage(params.__dict__[\"project_path\"]).model_storage_engine()\n",
    "    classifier = model_storage_engine.load(\"df7d10fa5605c9051bf509187c0f4efa\")\n",
    "    coef = classifier.coef_\n",
    "    c_sq = coef.squeeze()\n",
    "    max_fi.append(max(c_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fi = []\n",
    "\n",
    "slr_query = '''select distinct model_group_id from model_metadata.models where model_type = \n",
    "'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression' and run_time > to_timestamp('02 05 2019', 'MM-DD-YYYY'); '''\n",
    "\n",
    "results = engine.execute(slr_query)\n",
    "model_gp_id = [row[0] for row in results]\n",
    "model_gp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tuple_models2 = get_models_ids(model_gp_id, engine)\n",
    "l_t2 = [ModelEvaluator(i.model_group_id, i.model_id, engine) for i in list_tuple_models2]\n",
    "len(l_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for me in l_t2:\n",
    "    fi_slr = me._feature_importance_slr(params.__dict__[\"project_path\"])\n",
    "    df1 = fi_slr.sort_values(by=['coef_raw'], ascending = False).head(1)\n",
    "    df1[\"config\"] = repr(me)\n",
    "    max_fi.append(df1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coef = pd.concat(max_fi)\n",
    "top_coef.head()\n",
    "#top_coef.to_csv(\"top_fi_slr.csv\")\n",
    "top_coef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fi_slr = l_t[25]._feature_importance_slr(params.__dict__[\"project_path\"])\n",
    "    \n",
    "    model_storage_engine = ProjectStorage(params.__dict__[\"project_path\"]).model_storage_engine()\n",
    "    classifier = model_storage_engine.load(\"df7d10fa5605c9051bf509187c0f4efa\")\n",
    "    coef = classifier.coef_\n",
    "    c_sq = coef.squeeze()\n",
    "    max_fi.append(max(c_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_slr.sort_values(by=['coef_raw'], ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Another way of exploring this relationship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[3].plot_feature_importances_std_err(path= params.project_path, \n",
    "                                        n_features_plots = params.n_features_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[0].plot_feature_importances_std_err(path= params.project_path, \n",
    "                                        n_features_plots = 20,\n",
    "                                        bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which feature group has more relevance in my model performance? \n",
    "\n",
    "Feature inclusion and exclusion (LOI and LOO) performance allow us to compare different experiments made in each `model_group_id` and see how the exclusion or inclusion of new feature groups have a leverage in the user defined metric to assess performance. The method `feature_groups` depicts the expriment type for each of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[3].plot_feature_group_average_importances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a more detailed way, we can explore features and their distribution across label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l_t[0].plot_feature_distribution(path=params.project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "audited_models_class.feature_loi_loo(param_type='rank_pct',\n",
    "                                     param=10,\n",
    "                                     metric='precision@',\n",
    "                                     baseline=True,\n",
    "                                     baseline_query=params.baseline_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstabs: once predicted, what are the main difference between my groups? What's the feature that characterise my predictions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crosstabs is already on development, you can [check out it](https://github.com/dssg/postmodel-analysis/tree/sqlqueries/postmodel/crosstabs). `crosstabs` will generate a table with the differences in feature distributions across predicted entites using a top-k precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# edited name of table to test_results.crosstabs\n",
    "\n",
    "l_t[17].crosstabs_ratio_plot(n_features=params.n_features_plots, \n",
    "                            figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the model metrics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[6].plot_ROC(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[0].plot_recall_fpr_n(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[6].plot_precision_recall_n(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model make mistakes, why? \n",
    "\n",
    "We can explore which features of our space, or at which values, are some features generating errors in the model. This function `error_analysis` runs a Decision Tree for each model based in one (or several) thresholds passed to the Postmodeling configuration file. The function will return the tree plot (saved as pdf under the `/error_analysis`). \n",
    "\n",
    "The function will make *four* comparisons:\n",
    " - False Postives (1) vs. Rest of entities classified (0)\n",
    " - False Negatives (1) vs. Rest of entities classified (0)\n",
    " - False Positives (1) vs. True Negatives (0)\n",
    " - False Negatives (1) vs. True Positives (0)\n",
    " \n",
    "This set of analysis will serve to understand why some classification errors arise, and give some hints on how our model is performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t[0].error_analysis(params.thresholds_iterator,\n",
    "                      depth=params.max_depth_error_tree,\n",
    "                      path=params.project_path,\n",
    "                      view_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each `model_id` has different temporal configurations, we can explore the prediction window for each of the models and identify the models that have the same temporal configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's talk about model consistency\n",
    "\n",
    "We can compare `model_groups_id` overall by looking at how consistent they are in flagging the same individuals. We checked the temporal consistency inside each `model_group_id` in the precision/recall time line in the first part of this Notebook, but we can also compare `model_group_id`'s by the composition of both the predicted individuals in one especific time window, and its features. Here we present two ways of doing these using a Jaccard Similarity Index, and a Ranked Correlation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "audited_models_class.plot_jaccard(param_type='rank_pct',\n",
    "                                  param=10,\n",
    "                                  temporal_comparison=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audited_models_class.plot_ranked_corrlelation_preds(param_type='rank_abs',\n",
    "                                                    param=50,\n",
    "                                                    temporal_comparison=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, we can compare the selection of each model for each predicted `as_of_date` using the overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audited_models_class.plot_jaccard(param_type='rank_pct', \n",
    "                                  param=50,\n",
    "                                  figsize=params.figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistency also is desireable in the most relevant features for each of the models. We can explore this by exploring the correlation/overlap of ranked features across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audited_models_class.plot_jaccard_features(top_n_features=5,\n",
    "                                          temporal_comparison=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audited_models_class.plot_ranked_correlation_features(figsize=params.figsize,\n",
    "                                                      top_n_features=params.n_features_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model _\"advanced\"_ comparison\n",
    "\n",
    "More than the mere overlap of ranked observations, we can think our comparisons from the distributions of our top observations. Not only we can visualize the distribution overlap, but also the ranking of the predicted entities in their percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "audited_models_class.plot_preds_comparison(param_type='rank_abs',\n",
    "                                           model_subset=[125],\n",
    "                                           param=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees - Model Group 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triage.component.catwalk.storage import ProjectStorage\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "\n",
    "def draw_decision_tree(dt, feature_names):\n",
    "    export_graphviz(dt, out_file='tree.dot', \n",
    "                    feature_names = feature_names,\n",
    "                    class_names = ['0', '1'],\n",
    "                    rounded = True, proportion = False, \n",
    "                    precision = 2, filled = True)\n",
    "\n",
    "    # Convert to png using system command (requires Graphviz)    \n",
    "    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "    # Display in jupyter notebook\n",
    "    Image(filename = 'tree.png')\n",
    "\n",
    "query = \"select model_hash from model_metadata.models where model_group_id = {};\"\n",
    "\n",
    "\n",
    "results = engine.execute(query.format(7))\n",
    "model_hashes = [row[0] for row in results]\n",
    "\n",
    "query_tmid = \"select train_matrix_uuid from model_metadata.models where model_hash = '{}';\"\n",
    "\n",
    "for mh in model_hashes:\n",
    "    results_tm = engine.execute(query_tmid.format(mh))\n",
    "    train_hash = [row[0] for row in results_tm]\n",
    "    features = ProjectStorage(params.__dict__[\"project_path\"]).matrix_storage_engine().get_store(train_hash[0]).columns()\n",
    "    \n",
    "    model_storage_engine = ProjectStorage(params.__dict__[\"project_path\"]).model_storage_engine()\n",
    "    classifier = model_storage_engine.load(mh)\n",
    "    export_graphviz(classifier, out_file=\"mytree.dot\", feature_names = features)\n",
    "    with open(\"mytree.dot\") as f:\n",
    "        dot_graph = f.read()\n",
    "    graphviz.Source(dot_graph)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select train_matrix_uuid from model_metadata.models where model_hash = '{}';\"\n",
    "results_tm = engine.execute(query.format(model_hashes[0]))\n",
    "train_hash = [row[0] for row in results_tm]\n",
    "\n",
    "\n",
    "features = ProjectStorage(params.__dict__[\"project_path\"]).matrix_storage_engine().get_store(train_hash[0]).columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage_engine = ProjectStorage(params.__dict__[\"project_path\"]).model_storage_engine()\n",
    "classifier = model_storage_engine.load(model_hashes[0])\n",
    "\n",
    "export_graphviz(classifier, out_file=\"mytree.dot\", feature_names = features)\n",
    "with open(\"mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tm = engine.execute(query_tmid.format(model_hashes[5]))\n",
    "train_hash = [row[0] for row in results_tm]\n",
    "features = ProjectStorage(params.__dict__[\"project_path\"]).matrix_storage_engine().get_store(train_hash[0]).columns()\n",
    "\n",
    "model_storage_engine = ProjectStorage(params.__dict__[\"project_path\"]).model_storage_engine()\n",
    "classifier = model_storage_engine.load(model_hashes[1])\n",
    "export_graphviz(classifier, out_file=\"mytree.dot\", feature_names = features)\n",
    "with open(\"mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting precision at 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prec_10 = '''\n",
    "with mined_feat as (select ru.entity_id, ru.as_of_date, rural_entity_id_1y_rural_bool_max, overage_entity_id_1y_overage_bool_max, repeater_entity_id_1y_repeats_bool_max\n",
    "  from features.rural_aggregation_imputed as ru inner join features.overage_aggregation_imputed as ov \n",
    "  on ru.entity_id = ov.entity_id and ru.as_of_date = ov.as_of_date \n",
    "  inner join features.repeater_aggregation_imputed as re \n",
    "  on re.entity_id = ov.entity_id and re.as_of_date = ov.as_of_date\n",
    "  ),\n",
    "\n",
    "mined_label as (select entity_id, as_of_date, case\n",
    "  when (rural_entity_id_1y_rural_bool_max = 1) and (repeater_entity_id_1y_repeats_bool_max = 1) and (overage_entity_id_1y_overage_bool_max = 1)\n",
    "    then 1\n",
    "    else 0 end as mined_label\n",
    "    from mined_feat\n",
    "    ),\n",
    "\n",
    "acc_table as (select entity_id, as_of_date, mined_label, label, case\n",
    "  when mined_label = label\n",
    "    then 1\n",
    "    else 0 end as accuracy\n",
    "  from mined_label inner join semantic.label on \n",
    "  mined_label.entity_id::text = semantic.label.student and mined_label.as_of_date::date = semantic.label.event_date::date)\n",
    "\n",
    "select as_of_date, avg(label) as precision_10, 0 as model_group_id from (select * from (select ntile(10) over (partition by as_of_date order by mined_label desc) as pc, * from acc_table) AS ranked WHERE pc = 1) as top_10 group by as_of_date UNION\n",
    "\n",
    "select b.evaluation_end_time as as_of_date, b.value as precision_10,  a.model_group_id from (select model_group_id, model_id from model_metadata.models \n",
    "   where model_group_id =4 or model_group_id = 7 or model_group_id = 40 or model_group_id = 156) as a join \n",
    "   (select model_id, evaluation_end_time, value from test_results.evaluations where metric = 'precision@' and parameter = '10_pct') as b on a.model_id = b.model_id; \n",
    "'''\n",
    "\n",
    "results_prec_10 = engine.execute(query_prec_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rec_10 = '''\n",
    "with mined_feat as (select ru.entity_id, ru.as_of_date, rural_entity_id_1y_rural_bool_max, overage_entity_id_1y_overage_bool_max, repeater_entity_id_1y_repeats_bool_max\n",
    "  from features.rural_aggregation_imputed as ru inner join features.overage_aggregation_imputed as ov \n",
    "  on ru.entity_id = ov.entity_id and ru.as_of_date = ov.as_of_date \n",
    "  inner join features.repeater_aggregation_imputed as re \n",
    "  on re.entity_id = ov.entity_id and re.as_of_date = ov.as_of_date\n",
    "  ),\n",
    "\n",
    "mined_label as (select entity_id, as_of_date, case\n",
    "  when (rural_entity_id_1y_rural_bool_max = 1) and (repeater_entity_id_1y_repeats_bool_max = 1) and (overage_entity_id_1y_overage_bool_max = 1)\n",
    "    then 1\n",
    "    else 0 end as mined_label\n",
    "    from mined_feat\n",
    "    ),\n",
    "\n",
    "acc_table as (select entity_id, as_of_date, mined_label, label, case\n",
    "  when mined_label = label\n",
    "    then 1\n",
    "    else 0 end as accuracy\n",
    "  from mined_label inner join semantic.label on \n",
    "  mined_label.entity_id::text = semantic.label.student and mined_label.as_of_date::date = semantic.label.event_date::date)\n",
    "\n",
    "select num.as_of_date, (num.num_label::decimal)/denom.count as recall_10, 0 as model_group_id\n",
    "from (select as_of_date, sum(label) as num_label from (select * from (select ntile(10) over (partition by as_of_date order by mined_label desc) as pc, * from acc_table) AS ranked WHERE pc = 1) as top_10\n",
    "group by as_of_date) as num join (select event_date, sum(label) as count from semantic.label group by event_date) as denom on num.as_of_date = denom.event_date\n",
    "\n",
    "UNION\n",
    "\n",
    "select b.evaluation_end_time as as_of_date, b.value as recall_10, a.model_group_id from (select model_group_id, model_id from model_metadata.models \n",
    "   where model_group_id =4 or model_group_id = 7 or model_group_id = 40 or model_group_id = 156) as a join \n",
    "   (select model_id, evaluation_end_time, value from test_results.evaluations where metric = 'recall@' and parameter = '10_pct') as b on a.model_id = b.model_id; \n",
    "'''\n",
    "\n",
    "\n",
    "results_rec_10 = engine.execute(query_rec_10)\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision over time plot\n",
    "\n",
    "results_prec_10_df = pd.DataFrame(results_prec_10.fetchall())\n",
    "results_prec_10_df.columns = [\"as_of_date\", \"precision_10\", \"model_group_id\"]\n",
    "\n",
    "results_prec_10_df['model_group_id'].replace({0: 'decision_rule_baseline', 4: 'most_frequent_baseline', 7: 'decision_tree', \n",
    "            40: 'random_forest', 156: 'scaled_log_regression'}, inplace = True)\n",
    "\n",
    "results_prec_10_df['precision_10'] = pd.to_numeric(results_prec_10_df['precision_10'])\n",
    "\n",
    "results_prec_10_df = results_prec_10_df[results_prec_10_df['as_of_date'] != '2010-01-01']\n",
    "\n",
    "\n",
    "pr = ggplot(results_prec_10_df)+ aes('as_of_date', 'precision_10', color='model_group_id', group='model_group_id') + geom_point() + geom_line() + labs(title = \"Precision at 10% by Year\", x = \"Year\", y = \"Precision at 10%\") + scale_y_continuous(limits = [0,1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rec_10_df = pd.DataFrame(results_rec_10.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall over time plot\n",
    "\n",
    "results_rec_10_df = pd.DataFrame(results_rec_10.fetchall())\n",
    "results_rec_10_df.columns = [\"as_of_date\", \"recall_10\", \"model_group_id\"]\n",
    "\n",
    "\n",
    "results_rec_10_df['model_group_id'].replace({0: 'decision_rule_baseline', 4: 'most_frequent_baseline', 7: 'decision_tree', \n",
    "            40: 'random_forest', 156: 'scaled_log_regression'}, inplace = True)\n",
    "\n",
    "results_rec_10_df['recall_10'] = pd.to_numeric(results_rec_10_df['recall_10'])\n",
    "\n",
    "results_rec_10_df = results_rec_10_df[results_rec_10_df['as_of_date'] != '2010-01-01']\n",
    "\n",
    "#p = ggplot(results_rec_10_df) + aes('as_of_date', 'recall_10', color='model_group_id', group='model_group_id') + geom_point() + geom_line() + labs(title = \"Recall at 10% by Year\", x = \"Year\", y = \"Recall at 10%\") + scale_y_continuous(limits = [0,1])\n",
    "                      \n",
    "#p.save('recall_10.png', width = 10, height = 10, dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# side by side bars\n",
    "\n",
    "query_count_dropout = '''\n",
    "                    select event_date, sum(label) as count from semantic.label group by event_date;\n",
    "                    '''\n",
    "count_dropout = engine.execute(query_count_dropout)\n",
    "count_dropout_df = pd.DataFrame(count_dropout.fetchall())\n",
    "count_dropout_df.columns = [\"as_of_date\", \"count\"]\n",
    "\n",
    "count_dropout_df['as_of_date'] = pd.to_datetime(count_dropout_df['as_of_date'])\n",
    "df = pd.merge(results_rec_10_df, count_dropout_df, on = \"as_of_date\")\n",
    "df['num_reached'] = round(df['recall_10']*df['count'])\n",
    "df['model_group_id'].replace({0: 'decision_rule_baseline', 4: 'most_frequent_baseline', 7: 'decision_tree', \n",
    "            40: 'random_forest', 156: 'scaled_log_regression'}, inplace = True)\n",
    "\n",
    "df_gp = df.groupby('model_group_id').mean()\n",
    "df_gp = df_gp.reset_index()\n",
    "df_gp['num_reached'] = round(df_gp['num_reached'], 0)\n",
    "df_gp['num_reached'] = df_gp['num_reached'].apply(int)\n",
    "\n",
    "p = ggplot(df_gp, aes(x='model_group_id', y='num_reached', fill='model_group_id')) + geom_bar(stat='identity', position='dodge') + labs(title = \"Number of Dropouts Reached @ 10% of Population\", x = \"Model\", y = \"Number Dropouts Reached\") + geom_text(aes(label='num_reached'), position=position_dodge(width=0.5), nudge_y = 1000) + theme(axis_text_x=element_text(rotation=60, hjust=1))\n",
    "p.save('number_reached.png', width = 10, height = 10, dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rec_10_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_rule_baseline = [1821, 1049, 1536, 1317, 1976, 2159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = l_t[0].feature_importances(path=params.project_path)\n",
    "importances = importances.filter(items=['feature', 'feature_importance'])\n",
    "importances = importances.set_index('feature')\n",
    "\n",
    "# Sort by the absolute value of the importance of the feature\n",
    "importances['sort'] = abs(importances['feature_importance'])\n",
    "importances = \\\n",
    "        importances.sort_values(by='sort', ascending=False).drop('sort', axis=1)\n",
    "importances = importances[0:50]\n",
    "importances.reset_index(inplace = True)\n",
    "importances.rename(index=str, columns={\"feature\": \"feature\", \"feature_importance\": str(l_t[0].model_id)}, inplace = True)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of top 30 features from the three models,\n",
    "\n",
    "for model in l_t[1:]:\n",
    "    importances1 = model.feature_importances(path=params.project_path)\n",
    "    importances1 = importances1.filter(items=['feature', 'feature_importance'])\n",
    "    importances1 = importances1.set_index('feature')\n",
    "\n",
    "    # Sort by the absolute value of the importance of the feature\n",
    "    importances1['sort'] = abs(importances1['feature_importance'])\n",
    "    importances1 = \\\n",
    "            importances1.sort_values(by='sort', ascending=False).drop('sort', axis=1)\n",
    "    importances1 = importances1[0:50]\n",
    "    importances1.reset_index(inplace = True)\n",
    "    importances1.rename(index=str, columns={\"feature\": \"feature\", \"feature_importance\": str(model.model_id)}, inplace = True)\n",
    "\n",
    "    importances = pd.merge(importances, importances1, on='feature', how='outer')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null = importances.count(axis=1)\n",
    "importances['num_top_50'] = non_null\n",
    "# remove non-null count for feature name\n",
    "importances['num_top_50'] = importances['num_top_50'] -1\n",
    "importances.sort_values(by = 'num_top_50', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.to_csv('feat_imp_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = {70: 40, 156:40, 157:40, 158:40, 159:40, 160:40, 37:7, 220:7, 238:7, 256:7, 274:7, 292:7, 213:156, 231:156, \n",
    " 249:156, 267:156, 285:156, 303:156}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
